{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7977b244",
   "metadata": {},
   "source": [
    "Here we have tried modelling a CNN on a dataset of drug-side effect pair. But we only consider side effect groups which is 23 labels. And this is a multi-label classification problem where dataset is highly skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "id": "32e01a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037ede98",
   "metadata": {},
   "source": [
    "## For getting one-hot encoding for the smile strings (manual encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b12932",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smiles = pd.read_csv('STITCH_extended_isoSMILES.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cd0e232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 61)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "CN1C=NC2=C1C(=O)N(C(=O)N2C)C                                                                                                                                                                                                                                                                                                                                                                                    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# define SMILES characters ----------------------------------------------------\n",
    "SMILES_CHARS = [' ',\n",
    "                '#', '%', '(', ')', '+', '-', '.', '/',\n",
    "                '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "                '=', '@', ':','^', '|',\n",
    "                'A', 'B', 'C', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P',\n",
    "                'R', 'S', 'T', 'V', 'X', 'Z',\n",
    "                '[', '\\\\', ']',\n",
    "                'a', 'b', 'c', 'd','e', 'g', 'i', 'l', 'n', 'o', 'p', 'r', 's',\n",
    "                't', 'u']\n",
    "                \n",
    "# define encoder and decoder --------------------------------------------------\n",
    "smi2index = dict( (c,i) for i,c in enumerate( SMILES_CHARS ) )\n",
    "index2smi = dict( (i,c) for i,c in enumerate( SMILES_CHARS ) )\n",
    "\n",
    "def smiles_encoder( smiles, maxlen=400 ): \n",
    "    X = np.zeros( ( maxlen, len( SMILES_CHARS ) ) )\n",
    "    for i, c in enumerate( smiles ):\n",
    "        X[i, smi2index[c] ] = 1\n",
    "    return X # X will be the size of max sequence length * vocabulary size\n",
    "\n",
    "def smiles_decoder( X ):\n",
    "    smi = ''\n",
    "    X = X.argmax( axis=-1 )\n",
    "    for i in X:\n",
    "        smi += index2smi[ i ]\n",
    "    return smi\n",
    "\n",
    "# get a taste of caffeine -----------------------------------------------------\n",
    "caffeine_smiles = 'CN1C=NC2=C1C(=O)N(C(=O)N2C)C'\n",
    "\n",
    "caffeine_encoding = smiles_encoder(caffeine_smiles)\n",
    "\n",
    "print(caffeine_encoding.shape)\n",
    "print(caffeine_encoding)\n",
    "print(smiles_decoder(caffeine_encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cd9430",
   "metadata": {},
   "source": [
    "# Split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e29f1c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "efcaaa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.7\n",
    "VAL_SIZE = 0.15\n",
    "TEST_SIZE = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "cf9527fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(X, y, train_size):\n",
    "    \"\"\"Split dataset into data splits.\"\"\"\n",
    "#     X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y) # because of multilable classification, stratification is difficult as it needs more sample under each class\n",
    "#     X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)\n",
    "    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_, y_, train_size=0.5)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0472209",
   "metadata": {},
   "source": [
    "p.s.: The stratify parameter set it to split data in a way to allocate test_size amount of data to each class. In this case, you don't have sufficient class labels of one of your classes to keep the data splitting ratio equal to test_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "id": "88fc4cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>STITCH</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>Blood and lymphatic system disorders</th>\n",
       "      <th>Cardiac disorders</th>\n",
       "      <th>Congenital, familial and genetic disorders</th>\n",
       "      <th>Ear and labyrinth disorders</th>\n",
       "      <th>Endocrine disorders</th>\n",
       "      <th>Eye disorders</th>\n",
       "      <th>Gastrointestinal disorders</th>\n",
       "      <th>...</th>\n",
       "      <th>Musculoskeletal and connective tissue disorders</th>\n",
       "      <th>Neoplasms benign, malignant and unspecified (incl cysts and polyps)</th>\n",
       "      <th>Nervous system disorders</th>\n",
       "      <th>Pregnancy, puerperium and perinatal conditions</th>\n",
       "      <th>Psychiatric disorders</th>\n",
       "      <th>Renal and urinary disorders</th>\n",
       "      <th>Reproductive system and breast disorders</th>\n",
       "      <th>Respiratory, thoracic and mediastinal disorders</th>\n",
       "      <th>Skin and subcutaneous tissue disorders</th>\n",
       "      <th>Vascular disorders</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>CID000000143</td>\n",
       "      <td>Nc1nc(=O)c2c([nH]1)NCC(CNc1ccc(C(=O)NC(CCC(=O)O)C(=O)O)cc1)N2C=O</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>CID000000772</td>\n",
       "      <td>CC(=O)NC1C(O)OC(COS(=O)(=O)O)C(OC2OC(C(=O)O)C(OC3OC(CO)C(OC4OC(C(=O)O)C(O)C(O)C4OS(=O)(=O)O)C(OS(=O)(=O)O)C3NS(=O)(=O)O)C(O)C2OS(=O)(=O)O)C1O</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>CID000002019</td>\n",
       "      <td>Cc1c2oc3c(C)ccc(C(=O)NC4C(=O)NC(C(C)C)C(=O)N5CCCC5C(=O)N(C)CC(=O)N(C)C(C(C)C)C(=O)OC4C)c3nc-2c(C(=O)NC2C(=O)NC(C(C)C)C(=O)N3CCCC3C(=O)N(C)CC(=O)N(C)C(C(C)C)C(=O)OC2C)c(N)c1=O</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66</td>\n",
       "      <td>CID000002156</td>\n",
       "      <td>CCCCc1oc2ccccc2c1C(=O)c1cc(I)c(OCC[NH+](CC)CC)c(I)c1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70</td>\n",
       "      <td>CID000002162</td>\n",
       "      <td>CCOC(=O)C1=C(COCCN)NC(C)=C(C(=O)OC)C1c1ccccc1Cl</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>1549</td>\n",
       "      <td>CID070683024</td>\n",
       "      <td>COCCOC(=O)NCC(=O)N[C@H]1[C@H]([C@H](O)[C@H](O)CO)O[C@@](OC[C@H]2O[C@H](O[C@H](C)[C@H](N)C(=O)O)[C@H](NC(C)=O)[C@@H](O)[C@H]2O)(C(=O)O)C[C@@H]1O</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>1550</td>\n",
       "      <td>CID070685014</td>\n",
       "      <td>C[C@@H]1NC(=O)[C@@H]2CCCN2C(=O)[C@H](CC(N)=O)NC(=O)[C@@H]2CSSC[C@@H](N)C(=O)N[C@H]3CSSC[C@H](NC1=O)C(=O)N[C@@H]([C@@H](C)O)C(=O)NCC(=O)N[C@H](C(=O)N[C@@H](Cc1ccc(O)cc1)C(=O)O)CSSC[C@H](NC(=O)[C@H](Cc1ccc(O)cc1)NC(=O)[C@H](CCC(=O)O)NC3=O)C(=O)N2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>1552</td>\n",
       "      <td>CID070788982</td>\n",
       "      <td>NCCCC[C@@H]1NC(=O)[C@@H](Cc2c[nH]c3ccccc23)NC(=O)[C@H](c2ccccc2)NC(=O)[C@@H]2C[C@@H](OC(=O)NCCN)CN2C(=O)[C@H](Cc2ccccc2)NC(=O)[C@H](Cc2ccc(OCc3ccccc3)cc2)NC1=O.N[C@@H](CC(=O)O)C(=O)O.N[C@@H](CC(=O)O)C(=O)O</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>1553</td>\n",
       "      <td>CID071306410</td>\n",
       "      <td>CC(C)(C)NCC(O)COc1nsnc1N1CCOCC1.CCNC1CN(CCCOC)S(=O)(=O)c2sc(S(N)(=O)=O)cc21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>1554</td>\n",
       "      <td>CID071306834</td>\n",
       "      <td>CCCN(CCOc1c(Cl)cc(Cl)cc1Cl)C(=O)n1ccnc1.O=S(=O)([O-])C(I)I.[Na+]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>825 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0        STITCH  \\\n",
       "0             2  CID000000143   \n",
       "1            25  CID000000772   \n",
       "2            50  CID000002019   \n",
       "3            66  CID000002156   \n",
       "4            70  CID000002162   \n",
       "..          ...           ...   \n",
       "820        1549  CID070683024   \n",
       "821        1550  CID070685014   \n",
       "822        1552  CID070788982   \n",
       "823        1553  CID071306410   \n",
       "824        1554  CID071306834   \n",
       "\n",
       "                                                                                                                                                                                                                                                   SMILES  \\\n",
       "0                                                                                                                                                                                        Nc1nc(=O)c2c([nH]1)NCC(CNc1ccc(C(=O)NC(CCC(=O)O)C(=O)O)cc1)N2C=O   \n",
       "1                                                                                                           CC(=O)NC1C(O)OC(COS(=O)(=O)O)C(OC2OC(C(=O)O)C(OC3OC(CO)C(OC4OC(C(=O)O)C(O)C(O)C4OS(=O)(=O)O)C(OS(=O)(=O)O)C3NS(=O)(=O)O)C(O)C2OS(=O)(=O)O)C1O   \n",
       "2                                                                          Cc1c2oc3c(C)ccc(C(=O)NC4C(=O)NC(C(C)C)C(=O)N5CCCC5C(=O)N(C)CC(=O)N(C)C(C(C)C)C(=O)OC4C)c3nc-2c(C(=O)NC2C(=O)NC(C(C)C)C(=O)N3CCCC3C(=O)N(C)CC(=O)N(C)C(C(C)C)C(=O)OC2C)c(N)c1=O   \n",
       "3                                                                                                                                                                                                    CCCCc1oc2ccccc2c1C(=O)c1cc(I)c(OCC[NH+](CC)CC)c(I)c1   \n",
       "4                                                                                                                                                                                                         CCOC(=O)C1=C(COCCN)NC(C)=C(C(=O)OC)C1c1ccccc1Cl   \n",
       "..                                                                                                                                                                                                                                                    ...   \n",
       "820                                                                                                       COCCOC(=O)NCC(=O)N[C@H]1[C@H]([C@H](O)[C@H](O)CO)O[C@@](OC[C@H]2O[C@H](O[C@H](C)[C@H](N)C(=O)O)[C@H](NC(C)=O)[C@@H](O)[C@H]2O)(C(=O)O)C[C@@H]1O   \n",
       "821  C[C@@H]1NC(=O)[C@@H]2CCCN2C(=O)[C@H](CC(N)=O)NC(=O)[C@@H]2CSSC[C@@H](N)C(=O)N[C@H]3CSSC[C@H](NC1=O)C(=O)N[C@@H]([C@@H](C)O)C(=O)NCC(=O)N[C@H](C(=O)N[C@@H](Cc1ccc(O)cc1)C(=O)O)CSSC[C@H](NC(=O)[C@H](Cc1ccc(O)cc1)NC(=O)[C@H](CCC(=O)O)NC3=O)C(=O)N2   \n",
       "822                                         NCCCC[C@@H]1NC(=O)[C@@H](Cc2c[nH]c3ccccc23)NC(=O)[C@H](c2ccccc2)NC(=O)[C@@H]2C[C@@H](OC(=O)NCCN)CN2C(=O)[C@H](Cc2ccccc2)NC(=O)[C@H](Cc2ccc(OCc3ccccc3)cc2)NC1=O.N[C@@H](CC(=O)O)C(=O)O.N[C@@H](CC(=O)O)C(=O)O   \n",
       "823                                                                                                                                                                           CC(C)(C)NCC(O)COc1nsnc1N1CCOCC1.CCNC1CN(CCCOC)S(=O)(=O)c2sc(S(N)(=O)=O)cc21   \n",
       "824                                                                                                                                                                                      CCCN(CCOc1c(Cl)cc(Cl)cc1Cl)C(=O)n1ccnc1.O=S(=O)([O-])C(I)I.[Na+]   \n",
       "\n",
       "     Blood and lymphatic system disorders  Cardiac disorders  \\\n",
       "0                                       1                  0   \n",
       "1                                       1                  1   \n",
       "2                                       1                  1   \n",
       "3                                       1                  1   \n",
       "4                                       1                  1   \n",
       "..                                    ...                ...   \n",
       "820                                     1                  1   \n",
       "821                                     1                  1   \n",
       "822                                     1                  1   \n",
       "823                                     1                  1   \n",
       "824                                     1                  1   \n",
       "\n",
       "     Congenital, familial and genetic disorders  Ear and labyrinth disorders  \\\n",
       "0                                             1                            1   \n",
       "1                                             1                            1   \n",
       "2                                             1                            1   \n",
       "3                                             1                            1   \n",
       "4                                             1                            1   \n",
       "..                                          ...                          ...   \n",
       "820                                           1                            1   \n",
       "821                                           1                            1   \n",
       "822                                           1                            1   \n",
       "823                                           1                            1   \n",
       "824                                           1                            1   \n",
       "\n",
       "     Endocrine disorders  Eye disorders  Gastrointestinal disorders  ...  \\\n",
       "0                      1              1                           1  ...   \n",
       "1                      1              1                           1  ...   \n",
       "2                      1              1                           1  ...   \n",
       "3                      1              1                           1  ...   \n",
       "4                      1              1                           1  ...   \n",
       "..                   ...            ...                         ...  ...   \n",
       "820                    0              1                           1  ...   \n",
       "821                    0              1                           1  ...   \n",
       "822                    1              1                           1  ...   \n",
       "823                    1              1                           1  ...   \n",
       "824                    1              1                           1  ...   \n",
       "\n",
       "     Musculoskeletal and connective tissue disorders  \\\n",
       "0                                                  1   \n",
       "1                                                  1   \n",
       "2                                                  1   \n",
       "3                                                  1   \n",
       "4                                                  1   \n",
       "..                                               ...   \n",
       "820                                                1   \n",
       "821                                                1   \n",
       "822                                                1   \n",
       "823                                                1   \n",
       "824                                                1   \n",
       "\n",
       "     Neoplasms benign, malignant and unspecified (incl cysts and polyps)  \\\n",
       "0                                                                      1   \n",
       "1                                                                      1   \n",
       "2                                                                      1   \n",
       "3                                                                      1   \n",
       "4                                                                      1   \n",
       "..                                                                   ...   \n",
       "820                                                                    0   \n",
       "821                                                                    1   \n",
       "822                                                                    1   \n",
       "823                                                                    0   \n",
       "824                                                                    1   \n",
       "\n",
       "     Nervous system disorders  Pregnancy, puerperium and perinatal conditions  \\\n",
       "0                           1                                               1   \n",
       "1                           1                                               1   \n",
       "2                           1                                               1   \n",
       "3                           1                                               1   \n",
       "4                           1                                               1   \n",
       "..                        ...                                             ...   \n",
       "820                         1                                               1   \n",
       "821                         1                                               1   \n",
       "822                         1                                               1   \n",
       "823                         1                                               1   \n",
       "824                         1                                               1   \n",
       "\n",
       "     Psychiatric disorders  Renal and urinary disorders  \\\n",
       "0                        1                            1   \n",
       "1                        1                            1   \n",
       "2                        1                            1   \n",
       "3                        1                            1   \n",
       "4                        1                            1   \n",
       "..                     ...                          ...   \n",
       "820                      0                            0   \n",
       "821                      1                            1   \n",
       "822                      1                            1   \n",
       "823                      1                            1   \n",
       "824                      1                            1   \n",
       "\n",
       "     Reproductive system and breast disorders  \\\n",
       "0                                           1   \n",
       "1                                           1   \n",
       "2                                           1   \n",
       "3                                           1   \n",
       "4                                           1   \n",
       "..                                        ...   \n",
       "820                                         0   \n",
       "821                                         1   \n",
       "822                                         1   \n",
       "823                                         0   \n",
       "824                                         1   \n",
       "\n",
       "     Respiratory, thoracic and mediastinal disorders  \\\n",
       "0                                                  1   \n",
       "1                                                  1   \n",
       "2                                                  1   \n",
       "3                                                  1   \n",
       "4                                                  1   \n",
       "..                                               ...   \n",
       "820                                                1   \n",
       "821                                                1   \n",
       "822                                                1   \n",
       "823                                                1   \n",
       "824                                                1   \n",
       "\n",
       "     Skin and subcutaneous tissue disorders  Vascular disorders  \n",
       "0                                         1                   1  \n",
       "1                                         1                   1  \n",
       "2                                         1                   1  \n",
       "3                                         1                   1  \n",
       "4                                         1                   1  \n",
       "..                                      ...                 ...  \n",
       "820                                       1                   1  \n",
       "821                                       1                   1  \n",
       "822                                       1                   1  \n",
       "823                                       1                   1  \n",
       "824                                       1                   1  \n",
       "\n",
       "[825 rows x 26 columns]"
      ]
     },
     "execution_count": 875,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_csv('SIDER_with_isoSMILES.tsv', sep='\\t')\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "id": "a2f55a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_sample_df = df_data.sample(n=400, random_state=42) # for pilot dataset\n",
    "# random_sample_df = df_data.sample(n=400) # for pilot dataset\n",
    "# random_sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "id": "eb9e357b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(825, 23)\n",
      "(825,)\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "# X = random_sample_df[\"SMILES\"].values\n",
    "# y = random_sample_df.iloc[:, 3:26].values\n",
    "X = df_data[\"SMILES\"].values\n",
    "y = df_data.iloc[:, 3:26].values\n",
    "# X = df_data[\"SMILES\"].values\n",
    "# y = df_data.iloc[:, 4:27].values\n",
    "print(y.shape)\n",
    "print(X.shape)\n",
    "# print(y[:,1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "id": "4c3c2b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = max(1, max(len(sequence) for sequence in X))\n",
    "print(max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "a94e244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = 0\n",
    "# df_test = pd.read_csv('SIDER_with_isoSMILES.tsv', sep='\\t')\n",
    "# X_t = df_test[\"SMILES\"].values\n",
    "# y_t = df_test.iloc[:, 4:27].values\n",
    "# for i in range(y_t.shape[0]):\n",
    "#     if np.all(y_t[i] == 0):\n",
    "#         c += 1\n",
    "#         print(i)\n",
    "# print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "bfbbf888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 [378 380 369 336 345 395 398 399 354 396 397 399 372 391 342 399 384 372\n",
      " 368 379 392 394 398] 8737\n"
     ]
    }
   ],
   "source": [
    "##-- Calculate class occurance in the sampled data --##\n",
    "class_vectors = np.array(y)\n",
    "class_count = np.sum(class_vectors, axis=0)\n",
    "print(len(class_count),  class_count, np.sum(class_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "id": "9e83b213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10.0000],\n",
      "        [ 0.5263]])\n"
     ]
    }
   ],
   "source": [
    "## -- one way of calculating class weights --##\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.autograd import Variable\n",
    "def class_weights(dataset):\n",
    "    Y = []\n",
    "    for i in range(len(dataset)):\n",
    "        Y.append(dataset[i][1])\n",
    "    Y = np.vstack(Y)\n",
    "\n",
    "    neg_weights = []\n",
    "    pos_weights = []\n",
    "    for i in range(Y.shape[1]):\n",
    "        w = compute_class_weight(class_weight =\"balanced\", classes = np.unique(Y[:, i]), y=Y[:, i]) # Estimate class weights for unbalanced datasets.\n",
    "        neg_weights.append(w[0]) # corresponds to negative of a label/class\n",
    "        pos_weights.append(w[1]) # corresponds to positive of a label/class\n",
    "    return torch.from_numpy(np.array([neg_weights, pos_weights])).type(torch.FloatTensor)\n",
    "#     return Variable(torch.from_numpy(np.array([neg_weights, pos_weights])).type(torch.FloatTensor),\n",
    "#                     requires_grad=False)\n",
    "\n",
    "a = class_weights(y)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "d0821709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (280,), y_train: (280, 23)\n",
      "X_val: (60,), y_val: (60, 23)\n",
      "X_test: (60,), y_test: (60, 23)\n",
      "Sample point: CC1=C(CC(=O)[O-])c2cc(F)ccc2/C1=C\\c1ccc(S(C)=O... → [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Create data splits\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\n",
    "    X=X, y=y, train_size=TRAIN_SIZE)\n",
    "print (f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print (f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "print (f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "print (f\"Sample point: {X_train[0]} → {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59552ee8",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2377057",
   "metadata": {},
   "source": [
    "Convert our text input data into token indices. This means that every token (we can decide what a token is char, word, sub-word, etc.) is mapped to a unique index which allows us to represent our text as an array of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "9813345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from more_itertools import take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "454972e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    def __init__(self, char_level, num_tokens=None,\n",
    "                 pad_token=\"<PAD>\", oov_token=\"<UNK>\",\n",
    "                 token_to_index=None):\n",
    "        self.char_level = char_level\n",
    "        self.separator = \"\" if self.char_level else \" \"\n",
    "        if num_tokens: num_tokens -= 2 # pad + unk tokens\n",
    "        self.num_tokens = num_tokens\n",
    "        self.pad_token = pad_token\n",
    "        self.oov_token = oov_token\n",
    "        if not token_to_index:\n",
    "            token_to_index = {pad_token: 0, oov_token: 1}\n",
    "        self.token_to_index = token_to_index\n",
    "        self.index_to_token = {v: k for k, v in self.token_to_index.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_index)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<Tokenizer(num_tokens={len(self)})>\"\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "        if not self.char_level:\n",
    "            texts = [text.split(\" \") for text in texts]\n",
    "        all_tokens = [token for text in texts for token in text]\n",
    "        counts = Counter(all_tokens).most_common(self.num_tokens)\n",
    "        self.min_token_freq = counts[-1][1]\n",
    "        for token, count in counts:\n",
    "            index = len(self)\n",
    "            self.token_to_index[token] = index\n",
    "            self.index_to_token[index] = token\n",
    "        return self\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            if not self.char_level:\n",
    "                text = text.split(\" \")\n",
    "            sequence = []\n",
    "            for token in text:\n",
    "                sequence.append(self.token_to_index.get(\n",
    "                    token, self.token_to_index[self.oov_token]))\n",
    "            sequences.append(np.asarray(sequence))\n",
    "        return sequences\n",
    "\n",
    "    def sequences_to_texts(self, sequences):\n",
    "        texts = []\n",
    "        for sequence in sequences:\n",
    "            text = []\n",
    "            for index in sequence:\n",
    "                text.append(self.index_to_token.get(index, self.oov_token))\n",
    "            texts.append(self.separator.join([token for token in text]))\n",
    "        return texts\n",
    "\n",
    "    def save(self, fp):\n",
    "        with open(fp, \"w\") as fp:\n",
    "            contents = {\n",
    "                \"char_level\": self.char_level,\n",
    "                \"oov_token\": self.oov_token,\n",
    "                \"token_to_index\": self.token_to_index\n",
    "            }\n",
    "            json.dump(contents, fp, indent=4, sort_keys=False)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, fp):\n",
    "        with open(fp, \"r\") as fp:\n",
    "            kwargs = json.load(fp=fp)\n",
    "        return cls(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "d69eeab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Tokenizer(num_tokens=10)>\n",
      "[('<PAD>', 0), ('<UNK>', 1), ('C', 2), ('N', 3), ('=', 4)]\n",
      "least freq token's freq: 2\n",
      "[array([2, 3, 7, 2, 4, 3, 2, 8, 4, 2, 7, 2, 5, 4, 9, 6, 3, 5, 2, 5, 4, 9,\n",
      "       6, 3, 8, 2, 6, 2])]\n"
     ]
    }
   ],
   "source": [
    "# -- test --#\n",
    "smiles = ['CN1C=NC2=C1C(=O)N(C(=O)N2C)C']\n",
    "tokenizer = Tokenizer(char_level=True, num_tokens=62)\n",
    "tokenizer.fit_on_texts(texts=smiles)\n",
    "VOCAB_SIZE = len(tokenizer)\n",
    "print (tokenizer)\n",
    "print (take(5, tokenizer.token_to_index.items()))\n",
    "print (f\"least freq token's freq: {tokenizer.min_token_freq}\") \n",
    "test = tokenizer.texts_to_sequences(smiles)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "id": "eae975a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Tokenizer(num_tokens=47)>\n",
      "[('<PAD>', 0), ('<UNK>', 1), ('C', 2), ('(', 3), (')', 4)]\n",
      "least freq token's freq: 1\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(char_level=True, num_tokens=62)\n",
    "# tokenizer.fit_on_texts(texts=X_train)\n",
    "tokenizer.fit_on_texts(texts=X)\n",
    "VOCAB_SIZE = len(tokenizer)\n",
    "print (tokenizer)\n",
    "print (take(5, tokenizer.token_to_index.items()))\n",
    "print (f\"least freq token's freq: {tokenizer.min_token_freq}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "id": "bc4b133f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text to indices:\n",
      "  (preprocessed) → Nc1nc(=O)c2c([nH]1)NCC(CNc1ccc(C(=O)NC(CCC(=O)O)C(=O)O)cc1)N2C=O\n",
      "  (tokenized) → [13  5 12 16  5  3 10  6  4  5 14  5  3  8 16 11  9 12  4 13  2  2  3  2\n",
      " 13  5 12  5  5  5  3  2  3 10  6  4 13  2  3  2  2  2  3 10  6  4  6  4\n",
      "  2  3 10  6  4  6  4  5  5 12  4 13 14  2 10  6]\n"
     ]
    }
   ],
   "source": [
    "# Convert texts to sequences of indices\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "# X_train = tokenizer.texts_to_sequences(X_train)\n",
    "# X_val = tokenizer.texts_to_sequences(X_val)\n",
    "# X_test = tokenizer.texts_to_sequences(X_test)\n",
    "# preprocessed_text = tokenizer.sequences_to_texts([X_train[0]])[0]\n",
    "preprocessed_text = tokenizer.sequences_to_texts([X[0]])[0]\n",
    "print (\"Text to indices:\\n\"\n",
    "    f\"  (preprocessed) → {preprocessed_text}\\n\"\n",
    "    f\"  (tokenized) → {X[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2ce93c",
   "metadata": {},
   "source": [
    "# One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636c6036",
   "metadata": {},
   "source": [
    "OHE of the tokens will create matrix with binary values where each vocabulary or tokens will be indicated by 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "id": "c6402a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(seq, num_classes):\n",
    "    \"\"\"One-hot encode a sequence of tokens.\"\"\"\n",
    "    one_hot = np.zeros((len(seq), num_classes)) # num_classes is the vocabulary size\n",
    "    for i, item in enumerate(seq):\n",
    "        one_hot[i, item] = 1.\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "6a82499c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  2 12 10  2  4  2  2  4 10  6  5  9  6 20 11  5  3 15  3  3  4 23  5\n",
      "  3  3  3 15 18  2 12 10  2 24  3 12  3  3  3  4 21  4  2  5 10  6  7  7\n",
      "  7]\n",
      "49\n",
      "[[0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(49, 38)\n"
     ]
    }
   ],
   "source": [
    "## -- test --##\n",
    "print (X_train[0]) # token for the first smile in the data\n",
    "print (len(X_train[0]))\n",
    "cat = to_categorical(seq=X_train[0], num_classes=len(tokenizer))\n",
    "print (cat)\n",
    "print (cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "id": "81beb37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokens to one-hot\n",
    "vocab_size = len(tokenizer)\n",
    "X = [to_categorical(seq, num_classes=vocab_size) for seq in X]\n",
    "# X_train = [to_categorical(seq, num_classes=vocab_size) for seq in X_train]\n",
    "# X_val = [to_categorical(seq, num_classes=vocab_size) for seq in X_val]\n",
    "# X_test = [to_categorical(seq, num_classes=vocab_size) for seq in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "id": "4a59a91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "print(len(X[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c38dfe4",
   "metadata": {},
   "source": [
    "# Padding\n",
    "Our inputs are all of varying length but we need each batch to be uniformly shaped. Therefore, we will use padding to make all the inputs in the batch the same length.\n",
    "\n",
    "Here we will create a batch of shape (N (i.e., sample size), max_seq_len, vocab_size) so we'll need to be able to pad 3D sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "id": "05a11005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, max_seq_len=0): # ACROSS DIFFERENT BATCHES WE CAN MAKE IT CONSISTENT, SAY max_seq_len=382\n",
    "    \"\"\"Pad sequences to max length in sequence.\"\"\"\n",
    "    max_seq_len = max(max_seq_len, max(len(sequence) for sequence in sequences))\n",
    "    num_classes = sequences[0].shape[-1]\n",
    "    padded_sequences = np.zeros((len(sequences), max_seq_len, num_classes))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        padded_sequences[i][:len(sequence)] = sequence\n",
    "    return padded_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "id": "d54de6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 47) (141, 47) (174, 47)\n",
      "(3, 174, 47)\n"
     ]
    }
   ],
   "source": [
    "# -- Test -- #\n",
    "print (X[0].shape, X[1].shape, X[2].shape)\n",
    "padded = pad_sequences(X[0:3])\n",
    "print (padded.shape) # (N (i.e., sample size), max_seq_len, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ceb9c3",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "ef132020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "ec3f7a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "5\n",
      "<Dataset(N=5)>\n",
      "torch.Size([280, 23])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "(280, 23)\n",
      "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "torch.Size([280, 23])\n"
     ]
    }
   ],
   "source": [
    "# -- test--#\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"<Dataset(N={len(self)})>\"\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Create some example data\n",
    "data = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Create an instance of the custom dataset\n",
    "dataset = CustomDataset(data)\n",
    "\n",
    "# Access individual samples using indexing\n",
    "print(dataset[0])  # Output: 1\n",
    "print(dataset[2])  # Output: 3\n",
    "print(len(dataset))\n",
    "print(str(dataset))\n",
    "\n",
    "\n",
    "# y = torch.LongTensor(y_train.astype(np.int32))\n",
    "y = torch.LongTensor(y_train)\n",
    "print(y.shape)\n",
    "print(y[0])\n",
    "print(y_train.shape)\n",
    "\n",
    "# X = pad_sequences(X_train[0:3], max_seq_len=1)\n",
    "\n",
    "# # X = torch.FloatTensor(np.array(X).astype(np.int32)) # float because input is kind of continuous in the pixel space\n",
    "# # #         y = torch.LongTensor(y.astype(np.int32)) # integer casting of labels as we only have 0 and 1\n",
    "y = torch.LongTensor(np.array(y_train).astype(np.int32))\n",
    "print(y)\n",
    "print(y.shape)\n",
    "# # print(np.array(X).astype(np.int32))\n",
    "# print(X, y)\n",
    "# print(X_train[0].shape, X_train[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "c09e9cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, max_filter_size):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.max_filter_size = max_filter_size # this is padding arguement\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<Dataset(N={len(self)})>\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[index]\n",
    "        y = self.y[index]\n",
    "        return X, y\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"Custom collate function.\"\"\"\n",
    "        batch_X = [item[0] for item in batch]\n",
    "        batch_y = [item[1] for item in batch]\n",
    "\n",
    "        # Pad sequences (if necessary)\n",
    "        batch_X = pad_sequences(batch_X, max_seq_len=self.max_filter_size) # padding for consistent dimensions for each input batch\n",
    "\n",
    "\n",
    "        # Convert batch_y to tensors and reshape\n",
    "#         batch_y = torch.stack([torch.LongTensor(y).view(1, -1) for y in batch_y], dim=0)\n",
    "        batch_y = torch.stack([torch.FloatTensor(y).view(1, -1) for y in batch_y], dim=0)\n",
    "\n",
    "        # Cast\n",
    "        batch_X = torch.FloatTensor(batch_X) # float because input is kind of continuous in the pixel space\n",
    "    #     batch_y = torch.LongTensor(batch_y)\n",
    "\n",
    "        return batch_X, batch_y\n",
    "\n",
    "\n",
    "    def create_dataloader(self, batch_size, shuffle=False, drop_last=True):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset=self, batch_size=batch_size, collate_fn=self.collate_fn,\n",
    "            shuffle=shuffle, drop_last=drop_last, pin_memory=True) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0f1f64",
   "metadata": {},
   "source": [
    "drop_last: If set to True, it drops the last incomplete batch if its size is less than the specified batch size. This is commonly used to ensure all batches have the same size.\n",
    "\n",
    "pin_memory: If set to True, data loader copies Tensors into pinned memory before returning them. This can speed up GPU transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "10acb249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets:\n",
      "  Train dataset:<Dataset(N=280)>\n",
      "  Val dataset: <Dataset(N=60)>\n",
      "  Test dataset: <Dataset(N=60)>\n",
      "Sample point:\n",
      "  X: [[0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "  y: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " X_test.shape: (49, 38)\n",
      " Y_test.shape: (23,)\n"
     ]
    }
   ],
   "source": [
    "# Create datasets for embedding\n",
    "train_dataset = Dataset(X=X_train, y=y_train, max_filter_size=1)\n",
    "val_dataset = Dataset(X=X_val, y=y_val, max_filter_size=1)\n",
    "test_dataset = Dataset(X=X_test, y=y_test, max_filter_size=1)\n",
    "print (\"Datasets:\\n\"\n",
    "    f\"  Train dataset:{str(train_dataset)}\\n\"\n",
    "    f\"  Val dataset: {str(val_dataset)}\\n\"\n",
    "    f\"  Test dataset: {str(test_dataset)}\\n\"\n",
    "    \"Sample point:\\n\"\n",
    "    f\"  X: {test_dataset[0][0]}\\n\"\n",
    "    f\"  y: {test_dataset[0][1]}\\n\"\n",
    "       f\" X_test.shape: {test_dataset[0][0].shape}\\n\"\n",
    "       f\" Y_test.shape: {test_dataset[0][1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "1c3a5fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over batches\n",
    "# for batch_X, batch_y in dataloader:\n",
    "#     # Print shape of y in the current batch\n",
    "#     print(\"Shape of y in current batch:\", batch_y.shape)\n",
    "#     print(\"Shape of x in current batch:\", batch_X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "aaf3d460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch:\n",
      "  X: [16, 49, 38]\n",
      "  y: [16, 1, 23]\n",
      "Sample point:\n",
      "  X: tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "  y: tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1.]])\n",
      "torch.Size([16, 1, 23])\n",
      "torch.Size([16, 23])\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "batch_size = 16\n",
    "train_dataloader = train_dataset.create_dataloader(batch_size=batch_size)\n",
    "val_dataloader = val_dataset.create_dataloader(batch_size=batch_size)\n",
    "test_dataloader = test_dataset.create_dataloader(batch_size=batch_size)\n",
    "batch_X, batch_y = next(iter(test_dataloader))\n",
    "print (\"Sample batch:\\n\"\n",
    "    f\"  X: {list(batch_X.size())}\\n\"\n",
    "    f\"  y: {list(batch_y.size())}\\n\"\n",
    "    \"Sample point:\\n\"\n",
    "    f\"  X: {batch_X[0]}\\n\"\n",
    "    f\"  y: {batch_y[0]}\")\n",
    "print(batch_y.shape)\n",
    "y_new = batch_y.squeeze(1)\n",
    "print(y_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "7a973e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_X, batch_y = next(iter(test_dataloader))\n",
    "# inputs, y_true = batch_X, batch_y\n",
    "# print(batch_y)\n",
    "# for i, batch in enumerate(test_dataloader):\n",
    "#     inputs, y_true = batch\n",
    "#     print(y_true)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafe047e",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "6f446b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "id": "15e1485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, num_filters, filter_size,\n",
    "                 hidden_dim, dropout_p, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Convolutional filters\n",
    "        self.filter_size = filter_size # we'll used 1d filters like 1x3\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=vocab_size, out_channels=num_filters,\n",
    "            kernel_size=filter_size, stride=1, padding=0, padding_mode=\"zeros\") # padding 0 is no padding\n",
    "        self.batch_norm = nn.BatchNorm1d(num_features=num_filters) \n",
    "\n",
    "        # FC layers\n",
    "        self.fc1 = nn.Linear(num_filters, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()  # Sigmoid activation function for multi-label classification\n",
    "\n",
    "    def forward(self, inputs, channel_first=False,):\n",
    "\n",
    "        # Rearrange input so num_channels is in dim 1 (N, C, L)\n",
    "        # With PyTorch, when dealing with convolution, our inputs (X) need to have the \n",
    "        # channels as the second dimension, so our inputs will be (N, vocab_size, max_seq_len)\n",
    "        x_in, = inputs\n",
    "        if not channel_first:\n",
    "            x_in = x_in.transpose(1, 2)\n",
    "\n",
    "        # Padding for `SAME` padding\n",
    "        max_seq_len = x_in.shape[2] # after transpose, the 3rd dim (i.e. 2) is the seq length\n",
    "        padding_left = int((self.conv.stride[0]*(max_seq_len-1) - max_seq_len + self.filter_size)/2)\n",
    "        padding_right = int(math.ceil((self.conv.stride[0]*(max_seq_len-1) - max_seq_len + self.filter_size)/2))\n",
    "\n",
    "        # Conv outputs\n",
    "        z = self.conv(F.pad(x_in, (padding_left, padding_right)))\n",
    "        z = self.batch_norm(z)  # Batch normalization\n",
    "        z = F.relu(z)  # Activation function\n",
    "        z = F.max_pool1d(z, z.size(2)).squeeze(2) # a 1-dimensional max pooling operation to the input tensor z along its last dimension\n",
    "\n",
    "        # FC layer\n",
    "        z = self.fc1(z)\n",
    "        z = self.dropout(z)\n",
    "        z = self.fc2(z)\n",
    "        z = self.sigmoid(z)  # Sigmoid activation function for multi-label classification\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "id": "70ac212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FILTERS = 50\n",
    "HIDDEN_DIM = 300\n",
    "DROPOUT_P = 0.2\n",
    "FILTER_SIZE = 3\n",
    "NUM_CLASSES = test_dataset[0][1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "id": "655f637d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "cuda = True\n",
    "device = torch.device(\"cuda\" if (\n",
    "    torch.cuda.is_available() and cuda) else \"cpu\")\n",
    "torch.set_default_tensor_type(\"torch.FloatTensor\")\n",
    "if device.type == \"cuda\":\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "id": "1da83b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.named_parameters of CNN(\n",
      "  (conv): Conv1d(47, 50, kernel_size=(3,), stride=(1,))\n",
      "  (batch_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=50, out_features=300, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=300, out_features=23, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = CNN(vocab_size=VOCAB_SIZE, num_filters=NUM_FILTERS, filter_size=FILTER_SIZE,\n",
    "            hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)\n",
    "model = model.to(device) # set device\n",
    "print (model.named_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de51358",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "ea05d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "id": "9285f6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- Trainer class to run single or multi-CV training (both exmaples are covered at the following)-- #\n",
    "class Trainer(object):\n",
    "    def __init__(self, model, device, loss_fn=None, optimizer=None, scheduler=None):\n",
    "\n",
    "        # Set params\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "    def train_step(self, dataloader):\n",
    "        \"\"\"Train step.\"\"\"\n",
    "        # Set model to train mode\n",
    "        self.model.train()\n",
    "        loss = 0.0\n",
    "\n",
    "        # Iterate over train batches\n",
    "        for i, batch in enumerate(dataloader):\n",
    "\n",
    "            # Step\n",
    "            batch = [item.to(self.device) for item in batch]  # Set device\n",
    "            inputs, targets = batch[:-1], batch[-1]\n",
    "            self.optimizer.zero_grad()  # Reset gradients\n",
    "            z = self.model(inputs)  # Forward pass\n",
    "#             J = self.loss_fn(z, targets)  # Define loss\n",
    "            J = self.loss_fn(z, targets.squeeze(1))  # Define loss\n",
    "            J.backward()  # Backward pass\n",
    "            self.optimizer.step()  # Update weights\n",
    "\n",
    "            # Cumulative Metrics\n",
    "            loss += (J.detach().item() - loss) / (i + 1)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def eval_step(self, dataloader):\n",
    "        \"\"\"Validation or test step.\"\"\"\n",
    "        # Set model to eval mode\n",
    "        self.model.eval()\n",
    "        loss = 0.0\n",
    "        y_trues, y_probs = [], []\n",
    "\n",
    "        # Iterate over val batches\n",
    "        with torch.inference_mode():\n",
    "            for i, batch in enumerate(dataloader):\n",
    "\n",
    "                # Step\n",
    "                batch = [item.to(self.device) for item in batch]  # Set device\n",
    "                inputs, y_true = batch[:-1], batch[-1]\n",
    "\n",
    "                z = self.model(inputs)  # Forward pass\n",
    "#                 J = self.loss_fn(z, y_true).item()\n",
    "                J = self.loss_fn(z, y_true.squeeze(1)).item()\n",
    "\n",
    "                # Cumulative Metrics\n",
    "                loss += (J - loss) / (i + 1)\n",
    "\n",
    "                # Store outputs\n",
    "#                 y_prob = F.softmax(z).cpu().numpy() # probably we don't need this as we already put sigmoid on the output layer\n",
    "                y_prob = z.detach().numpy() # probably this is how we cast tensor to numpy\n",
    "                y_probs.extend(y_prob)\n",
    "                y_trues.extend(y_true.cpu().numpy())\n",
    "\n",
    "        return loss, np.vstack(y_trues), np.vstack(y_probs)\n",
    "\n",
    "    def predict_step(self, dataloader):\n",
    "        \"\"\"Prediction step.\"\"\"\n",
    "        # Set model to eval mode\n",
    "        self.model.eval()\n",
    "        y_probs = []\n",
    "\n",
    "        # Iterate over val batches\n",
    "        with torch.inference_mode():\n",
    "            for i, batch in enumerate(dataloader):\n",
    "\n",
    "                # Forward pass w/ inputs\n",
    "                inputs, targets = batch[:-1], batch[-1]\n",
    "                z = self.model(inputs)\n",
    "\n",
    "                # Store outputs\n",
    "                y_prob = F.softmax(z).cpu().numpy()\n",
    "                y_probs.extend(y_prob)\n",
    "\n",
    "        return np.vstack(y_probs)\n",
    "\n",
    "    def train(self, num_epochs, patience, train_dataloader, val_dataloader):\n",
    "        history = {'train_loss': [], 'test_loss': []}\n",
    "        best_val_loss = np.inf\n",
    "        for epoch in range(num_epochs):\n",
    "            # Steps\n",
    "            train_loss = self.train_step(dataloader=train_dataloader)\n",
    "            val_loss, _, _ = self.eval_step(dataloader=val_dataloader)\n",
    "            self.scheduler.step(val_loss)\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = self.model\n",
    "                _patience = patience  # reset _patience\n",
    "            else:\n",
    "                _patience -= 1\n",
    "            if not _patience:  # 0\n",
    "                print(\"Stopping early!\")\n",
    "                break\n",
    "\n",
    "            # Logging\n",
    "            print(\n",
    "                f\"Epoch: {epoch+1} | \"\n",
    "                f\"train_loss: {train_loss:.5f}, \"\n",
    "                f\"val_loss: {val_loss:.5f}, \"\n",
    "                f\"lr: {self.optimizer.param_groups[0]['lr']:.2E}, \"\n",
    "                f\"_patience: {_patience}\"\n",
    "            )\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['test_loss'].append(val_loss)\n",
    "        avg_train_loss = np.mean(history['train_loss'])\n",
    "        avg_test_loss = np.mean(history['test_loss'])\n",
    "#         print(avg_train_loss,avg_test_loss)\n",
    "        return best_model, avg_train_loss, avg_test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664cad2f",
   "metadata": {},
   "source": [
    "## for single CV training that uses the previously done train_test_split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "id": "e412406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "PATIENCE = 5\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "id": "93679da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is necessary as our dataset is massively skewed to positives for all classes\n",
    "def calculate_pos_weights(data): # data = labels\n",
    "#     class_vectors = np.array(data)\n",
    "    class_counts = np.sum(data, axis=0)\n",
    "#     pos_weights = np.ones_like(class_counts)\n",
    "    neg_counts = [len(data)-pos_count for pos_count in class_counts]\n",
    "    pos_weights = []\n",
    "    for i in range(len(class_counts)):\n",
    "        pos_weights.append(neg_counts[i]/ class_counts[i] + 1e-5)\n",
    "    return torch.as_tensor(np.array(pos_weights), dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "id": "4910bf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0550, 0.0591, 0.0827, 0.2009, 0.1587, 0.0300, 0.0061, 0.0073, 0.1411,\n",
      "        0.0160, 0.0123, 0.0037, 0.0813, 0.0274, 0.1702, 0.0135, 0.0404, 0.0645,\n",
      "        0.0870, 0.0673, 0.0274, 0.0160, 0.0061])\n"
     ]
    }
   ],
   "source": [
    "class_weights = calculate_pos_weights(data=y) # positive weights\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "id": "9f521895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss\n",
    "# class_weights_tensor = torch.Tensor(list(class_weights.values())).to(device)\n",
    "# loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "id": "56af8eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer & scheduler\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.1, patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a72c589",
   "metadata": {},
   "source": [
    "The ReduceLROnPlateau scheduler in PyTorch adjusts the learning rate when a metric has stopped improving. \n",
    "\n",
    "optimizer: The optimizer for which the learning rate will be adjusted based on the specified metric.\n",
    "\n",
    "mode: Specifies whether to monitor the metric for improvement by 'min' or 'max'. If 'min', the learning rate will be reduced when the monitored metric has stopped decreasing; if 'max', it will be reduced when the monitored metric has stopped increasing.\n",
    "\n",
    "factor: The factor by which the learning rate will be reduced. For example, if factor=0.1, the learning rate will be multiplied by 0.1.\n",
    "\n",
    "patience: The number of epochs with no improvement after which the learning rate will be reduced. If set to 3, for example, the learning rate will be reduced after 3 epochs with no improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "id": "85db8e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer module\n",
    "trainer = Trainer(\n",
    "    model=model, device=device, loss_fn=loss_fn,\n",
    "    optimizer=optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "id": "a5f28810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.07757, val_loss: 0.07692, lr: 1.00E-03, _patience: 5\n",
      "Epoch: 2 | train_loss: 0.07603, val_loss: 0.07680, lr: 1.00E-03, _patience: 5\n",
      "Epoch: 3 | train_loss: 0.07433, val_loss: 0.07637, lr: 1.00E-03, _patience: 5\n",
      "Epoch: 4 | train_loss: 0.07305, val_loss: 0.07587, lr: 1.00E-03, _patience: 5\n",
      "Epoch: 5 | train_loss: 0.07147, val_loss: 0.07624, lr: 1.00E-03, _patience: 4\n",
      "Epoch: 6 | train_loss: 0.07023, val_loss: 0.07619, lr: 1.00E-03, _patience: 3\n",
      "Epoch: 7 | train_loss: 0.07002, val_loss: 0.07574, lr: 1.00E-03, _patience: 5\n",
      "Epoch: 8 | train_loss: 0.06873, val_loss: 0.07624, lr: 1.00E-03, _patience: 4\n",
      "Epoch: 9 | train_loss: 0.06848, val_loss: 0.07667, lr: 1.00E-03, _patience: 3\n",
      "Epoch: 10 | train_loss: 0.06757, val_loss: 0.07767, lr: 1.00E-03, _patience: 2\n",
      "Epoch: 11 | train_loss: 0.06670, val_loss: 0.07744, lr: 1.00E-04, _patience: 1\n",
      "Stopping early!\n",
      "0.07129105634310028 0.07656022476201708\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "best_model = trainer.train(\n",
    "    NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "id": "e6591695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of CNN(\n",
      "  (conv): Conv1d(38, 50, kernel_size=(3,), stride=(1,))\n",
      "  (batch_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (fc2): Linear(in_features=100, out_features=23, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(best_model.parameters)\n",
    "# print(best_model.eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e18a387",
   "metadata": {},
   "source": [
    "# K-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "id": "ff767fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "from torch.utils.data import random_split,SubsetRandomSampler, ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "id": "8ab7a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "splits=KFold(n_splits=k,shuffle=True,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "id": "1ed61943",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this class includes sampling from different folds ##\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, max_filter_size):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.max_filter_size = max_filter_size # this is padding arguement\n",
    "#         self.sampler = sampler # sampling list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<Dataset(N={len(self)})>\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[index]\n",
    "        y = self.y[index]\n",
    "        return X, y\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"Custom collate function.\"\"\"\n",
    "        batch_X = [item[0] for item in batch]\n",
    "        batch_y = [item[1] for item in batch]\n",
    "\n",
    "        # Pad sequences (if necessary)\n",
    "        batch_X = pad_sequences(batch_X, max_seq_len=self.max_filter_size) # padding for consistent dimensions for each input batch\n",
    "\n",
    "\n",
    "        # Convert batch_y to tensors and reshape\n",
    "#         batch_y = torch.stack([torch.LongTensor(y).view(1, -1) for y in batch_y], dim=0)\n",
    "        batch_y = torch.stack([torch.FloatTensor(y).view(1, -1) for y in batch_y], dim=0)\n",
    "\n",
    "        # Cast\n",
    "        batch_X = torch.FloatTensor(batch_X) # float because input is kind of continuous in the pixel space\n",
    "    #     batch_y = torch.LongTensor(batch_y)\n",
    "\n",
    "        return batch_X, batch_y\n",
    "\n",
    "\n",
    "    def create_dataloader(self, batch_size, sampler, shuffle=False, drop_last=True):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset=self, batch_size=batch_size, sampler=sampler, collate_fn=self.collate_fn,\n",
    "            shuffle=shuffle, drop_last=drop_last, pin_memory=True) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "id": "674c50e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets:\n",
      "  Train dataset:<Dataset(N=825)>\n",
      "Sample point:\n",
      "  X: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "  y: [1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " X.shape: (64, 47)\n",
      " Y.shape: (23,)\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(X=X, y=y, max_filter_size=400)\n",
    "print (\"Datasets:\\n\"\n",
    "    f\"  Train dataset:{str(dataset)}\\n\"\n",
    "    \"Sample point:\\n\"\n",
    "    f\"  X: {dataset[0][0]}\\n\"\n",
    "    f\"  y: {dataset[0][1]}\\n\"\n",
    "       f\" X.shape: {dataset[0][0].shape}\\n\"\n",
    "       f\" Y.shape: {dataset[0][1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "id": "dc44f3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0550, 0.0591, 0.0827, 0.2009, 0.1587, 0.0300, 0.0061, 0.0073, 0.1411,\n",
      "        0.0160, 0.0123, 0.0037, 0.0813, 0.0274, 0.1702, 0.0135, 0.0404, 0.0645,\n",
      "        0.0870, 0.0673, 0.0274, 0.0160, 0.0061])\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "PATIENCE = 10\n",
    "NUM_EPOCHS = 40\n",
    "\n",
    "# --define class weights\n",
    "class_weights = calculate_pos_weights(data=y) # positive weights\n",
    "print(class_weights)\n",
    "\n",
    "#--Define Loss\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "\n",
    "#--Define optimizer & scheduler\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.1, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "id": "03fda3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Sample batch:\n",
      "  X: [32, 400, 47]\n",
      "  y: [32, 1, 23]\n",
      "\n",
      "Epoch: 1 | train_loss: 0.07371, val_loss: 0.07915, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 2 | train_loss: 0.07362, val_loss: 0.07950, lr: 1.00E-08, _patience: 9\n",
      "Epoch: 3 | train_loss: 0.07411, val_loss: 0.07253, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 4 | train_loss: 0.07345, val_loss: 0.07563, lr: 1.00E-08, _patience: 9\n",
      "Epoch: 5 | train_loss: 0.07417, val_loss: 0.07595, lr: 1.00E-08, _patience: 8\n",
      "Epoch: 6 | train_loss: 0.07412, val_loss: 0.07208, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 7 | train_loss: 0.07412, val_loss: 0.08335, lr: 1.00E-08, _patience: 9\n",
      "Epoch: 8 | train_loss: 0.07360, val_loss: 0.07875, lr: 1.00E-08, _patience: 8\n",
      "Epoch: 9 | train_loss: 0.07418, val_loss: 0.07806, lr: 1.00E-08, _patience: 7\n",
      "Epoch: 10 | train_loss: 0.07377, val_loss: 0.08375, lr: 1.00E-08, _patience: 6\n",
      "Epoch: 11 | train_loss: 0.07363, val_loss: 0.08586, lr: 1.00E-08, _patience: 5\n",
      "Epoch: 12 | train_loss: 0.07371, val_loss: 0.07585, lr: 1.00E-08, _patience: 4\n",
      "Epoch: 13 | train_loss: 0.07398, val_loss: 0.06625, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 14 | train_loss: 0.07404, val_loss: 0.07799, lr: 1.00E-08, _patience: 9\n",
      "Epoch: 15 | train_loss: 0.07410, val_loss: 0.08367, lr: 1.00E-08, _patience: 8\n",
      "Epoch: 16 | train_loss: 0.07361, val_loss: 0.07848, lr: 1.00E-08, _patience: 7\n",
      "Epoch: 17 | train_loss: 0.07392, val_loss: 0.07536, lr: 1.00E-08, _patience: 6\n",
      "Epoch: 18 | train_loss: 0.07351, val_loss: 0.07694, lr: 1.00E-08, _patience: 5\n",
      "Epoch: 19 | train_loss: 0.07413, val_loss: 0.07264, lr: 1.00E-08, _patience: 4\n",
      "Epoch: 20 | train_loss: 0.07411, val_loss: 0.07130, lr: 1.00E-08, _patience: 3\n",
      "Epoch: 21 | train_loss: 0.07410, val_loss: 0.07462, lr: 1.00E-08, _patience: 2\n",
      "Epoch: 22 | train_loss: 0.07396, val_loss: 0.07443, lr: 1.00E-08, _patience: 1\n",
      "Stopping early!\n",
      "Performance of 0 fold:\n",
      "Average Training Loss: 0.0739 \t Average Test Loss: 0.0769 \n",
      "Hamming Loss: 0.8668 \t mPrecision: 1.0000 \t mRecall: 0.0886 \t mF1: 0.1627 \t micro_auc: 0.5902 \t macros_auc: 0.7043 \n",
      "Fold 2\n",
      "Sample batch:\n",
      "  X: [32, 400, 47]\n",
      "  y: [32, 1, 23]\n",
      "\n",
      "Epoch: 1 | train_loss: 0.07438, val_loss: 0.06243, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 2 | train_loss: 0.07409, val_loss: 0.06857, lr: 1.00E-08, _patience: 9\n",
      "Epoch: 3 | train_loss: 0.07487, val_loss: 0.06790, lr: 1.00E-08, _patience: 8\n",
      "Epoch: 4 | train_loss: 0.07495, val_loss: 0.06530, lr: 1.00E-08, _patience: 7\n",
      "Epoch: 5 | train_loss: 0.07463, val_loss: 0.07263, lr: 1.00E-08, _patience: 6\n",
      "Epoch: 6 | train_loss: 0.07510, val_loss: 0.07040, lr: 1.00E-08, _patience: 5\n",
      "Epoch: 7 | train_loss: 0.07510, val_loss: 0.06822, lr: 1.00E-08, _patience: 4\n",
      "Epoch: 8 | train_loss: 0.07477, val_loss: 0.06931, lr: 1.00E-08, _patience: 3\n",
      "Epoch: 9 | train_loss: 0.07509, val_loss: 0.06772, lr: 1.00E-08, _patience: 2\n",
      "Epoch: 10 | train_loss: 0.07509, val_loss: 0.07436, lr: 1.00E-08, _patience: 1\n",
      "Stopping early!\n",
      "Performance of 1 fold:\n",
      "Average Training Loss: 0.0748 \t Average Test Loss: 0.0687 \n",
      "Hamming Loss: 0.8811 \t mPrecision: 1.0000 \t mRecall: 0.0814 \t mF1: 0.1506 \t micro_auc: 0.6520 \t macros_auc: 0.7043 \n",
      "Fold 3\n",
      "Sample batch:\n",
      "  X: [32, 400, 47]\n",
      "  y: [32, 1, 23]\n",
      "\n",
      "Epoch: 1 | train_loss: 0.07405, val_loss: 0.07810, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 2 | train_loss: 0.07372, val_loss: 0.08036, lr: 1.00E-08, _patience: 9\n",
      "Epoch: 3 | train_loss: 0.07394, val_loss: 0.08443, lr: 1.00E-08, _patience: 8\n",
      "Epoch: 4 | train_loss: 0.07356, val_loss: 0.07022, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 5 | train_loss: 0.07379, val_loss: 0.08026, lr: 1.00E-08, _patience: 9\n",
      "Epoch: 6 | train_loss: 0.07415, val_loss: 0.08291, lr: 1.00E-08, _patience: 8\n",
      "Epoch: 7 | train_loss: 0.07329, val_loss: 0.07926, lr: 1.00E-08, _patience: 7\n",
      "Epoch: 8 | train_loss: 0.07394, val_loss: 0.08583, lr: 1.00E-08, _patience: 6\n",
      "Epoch: 9 | train_loss: 0.07402, val_loss: 0.08346, lr: 1.00E-08, _patience: 5\n",
      "Epoch: 10 | train_loss: 0.07410, val_loss: 0.07476, lr: 1.00E-08, _patience: 4\n",
      "Epoch: 11 | train_loss: 0.07393, val_loss: 0.07290, lr: 1.00E-08, _patience: 3\n",
      "Epoch: 12 | train_loss: 0.07402, val_loss: 0.07819, lr: 1.00E-08, _patience: 2\n",
      "Epoch: 13 | train_loss: 0.07412, val_loss: 0.07834, lr: 1.00E-08, _patience: 1\n",
      "Stopping early!\n",
      "Performance of 2 fold:\n",
      "Average Training Loss: 0.0739 \t Average Test Loss: 0.0792 \n",
      "Hamming Loss: 0.8567 \t mPrecision: 1.0000 \t mRecall: 0.0796 \t mF1: 0.1474 \t micro_auc: 0.6084 \t macros_auc: 0.7496 \n",
      "Fold 4\n",
      "Sample batch:\n",
      "  X: [32, 400, 47]\n",
      "  y: [32, 1, 23]\n",
      "\n",
      "Epoch: 1 | train_loss: 0.07447, val_loss: 0.07355, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 2 | train_loss: 0.07416, val_loss: 0.06810, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 3 | train_loss: 0.07458, val_loss: 0.07024, lr: 1.00E-08, _patience: 9\n",
      "Epoch: 4 | train_loss: 0.07468, val_loss: 0.07331, lr: 1.00E-08, _patience: 8\n",
      "Epoch: 5 | train_loss: 0.07419, val_loss: 0.07258, lr: 1.00E-08, _patience: 7\n",
      "Epoch: 6 | train_loss: 0.07462, val_loss: 0.07813, lr: 1.00E-08, _patience: 6\n",
      "Epoch: 7 | train_loss: 0.07438, val_loss: 0.07739, lr: 1.00E-08, _patience: 5\n",
      "Epoch: 8 | train_loss: 0.07459, val_loss: 0.05945, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 9 | train_loss: 0.07434, val_loss: 0.07098, lr: 1.00E-08, _patience: 9\n",
      "Epoch: 10 | train_loss: 0.07455, val_loss: 0.07686, lr: 1.00E-08, _patience: 8\n",
      "Epoch: 11 | train_loss: 0.07445, val_loss: 0.07030, lr: 1.00E-08, _patience: 7\n",
      "Epoch: 12 | train_loss: 0.07455, val_loss: 0.07514, lr: 1.00E-08, _patience: 6\n",
      "Epoch: 13 | train_loss: 0.07453, val_loss: 0.06850, lr: 1.00E-08, _patience: 5\n",
      "Epoch: 14 | train_loss: 0.07468, val_loss: 0.07644, lr: 1.00E-08, _patience: 4\n",
      "Epoch: 15 | train_loss: 0.07453, val_loss: 0.07155, lr: 1.00E-08, _patience: 3\n",
      "Epoch: 16 | train_loss: 0.07439, val_loss: 0.07060, lr: 1.00E-08, _patience: 2\n",
      "Epoch: 17 | train_loss: 0.07451, val_loss: 0.07258, lr: 1.00E-08, _patience: 1\n",
      "Stopping early!\n",
      "Performance of 3 fold:\n",
      "Average Training Loss: 0.0745 \t Average Test Loss: 0.0721 \n",
      "Hamming Loss: 0.8628 \t mPrecision: 1.0000 \t mRecall: 0.0922 \t mF1: 0.1688 \t micro_auc: 0.6082 \t macros_auc: 0.7496 \n",
      "Fold 5\n",
      "Sample batch:\n",
      "  X: [32, 400, 47]\n",
      "  y: [32, 1, 23]\n",
      "\n",
      "Epoch: 1 | train_loss: 0.07557, val_loss: 0.06999, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 2 | train_loss: 0.07542, val_loss: 0.06609, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 3 | train_loss: 0.07562, val_loss: 0.06830, lr: 1.00E-08, _patience: 9\n",
      "Epoch: 4 | train_loss: 0.07544, val_loss: 0.06222, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 5 | train_loss: 0.07536, val_loss: 0.06194, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 6 | train_loss: 0.07533, val_loss: 0.06019, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 7 | train_loss: 0.07566, val_loss: 0.06363, lr: 1.00E-08, _patience: 9\n",
      "Epoch: 8 | train_loss: 0.07512, val_loss: 0.06527, lr: 1.00E-08, _patience: 8\n",
      "Epoch: 9 | train_loss: 0.07559, val_loss: 0.06313, lr: 1.00E-08, _patience: 7\n",
      "Epoch: 10 | train_loss: 0.07555, val_loss: 0.06617, lr: 1.00E-08, _patience: 6\n",
      "Epoch: 11 | train_loss: 0.07560, val_loss: 0.06114, lr: 1.00E-08, _patience: 5\n",
      "Epoch: 12 | train_loss: 0.07554, val_loss: 0.06135, lr: 1.00E-08, _patience: 4\n",
      "Epoch: 13 | train_loss: 0.07526, val_loss: 0.06017, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 14 | train_loss: 0.07519, val_loss: 0.06069, lr: 1.00E-08, _patience: 9\n",
      "Epoch: 15 | train_loss: 0.07531, val_loss: 0.06662, lr: 1.00E-08, _patience: 8\n",
      "Epoch: 16 | train_loss: 0.07546, val_loss: 0.06648, lr: 1.00E-08, _patience: 7\n",
      "Epoch: 17 | train_loss: 0.07532, val_loss: 0.06461, lr: 1.00E-08, _patience: 6\n",
      "Epoch: 18 | train_loss: 0.07568, val_loss: 0.06283, lr: 1.00E-08, _patience: 5\n",
      "Epoch: 19 | train_loss: 0.07509, val_loss: 0.06885, lr: 1.00E-08, _patience: 4\n",
      "Epoch: 20 | train_loss: 0.07564, val_loss: 0.06441, lr: 1.00E-08, _patience: 3\n",
      "Epoch: 21 | train_loss: 0.07535, val_loss: 0.06151, lr: 1.00E-08, _patience: 2\n",
      "Epoch: 22 | train_loss: 0.07526, val_loss: 0.06102, lr: 1.00E-08, _patience: 1\n",
      "Stopping early!\n",
      "Performance of 4 fold:\n",
      "Average Training Loss: 0.0754 \t Average Test Loss: 0.0639 \n",
      "Hamming Loss: 0.8818 \t mPrecision: 1.0000 \t mRecall: 0.0961 \t mF1: 0.1753 \t micro_auc: 0.6418 \t macros_auc: 0.7496 \n",
      "Fold 6\n",
      "Sample batch:\n",
      "  X: [32, 400, 47]\n",
      "  y: [32, 1, 23]\n",
      "\n",
      "Epoch: 1 | train_loss: 0.07412, val_loss: 0.07868, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 2 | train_loss: 0.07387, val_loss: 0.07095, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 3 | train_loss: 0.07402, val_loss: 0.07532, lr: 1.00E-08, _patience: 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | train_loss: 0.07396, val_loss: 0.07415, lr: 1.00E-08, _patience: 8\n",
      "Epoch: 5 | train_loss: 0.07429, val_loss: 0.07445, lr: 1.00E-08, _patience: 7\n",
      "Epoch: 6 | train_loss: 0.07416, val_loss: 0.07551, lr: 1.00E-08, _patience: 6\n",
      "Epoch: 7 | train_loss: 0.07410, val_loss: 0.07760, lr: 1.00E-08, _patience: 5\n",
      "Epoch: 8 | train_loss: 0.07424, val_loss: 0.07580, lr: 1.00E-08, _patience: 4\n",
      "Epoch: 9 | train_loss: 0.07425, val_loss: 0.07353, lr: 1.00E-08, _patience: 3\n",
      "Epoch: 10 | train_loss: 0.07413, val_loss: 0.07378, lr: 1.00E-08, _patience: 2\n",
      "Epoch: 11 | train_loss: 0.07381, val_loss: 0.07475, lr: 1.00E-08, _patience: 1\n",
      "Stopping early!\n",
      "Performance of 5 fold:\n",
      "Average Training Loss: 0.0741 \t Average Test Loss: 0.0750 \n",
      "Hamming Loss: 0.8492 \t mPrecision: 1.0000 \t mRecall: 0.0968 \t mF1: 0.1765 \t micro_auc: 0.6240 \t macros_auc: 0.7496 \n",
      "Fold 7\n",
      "Sample batch:\n",
      "  X: [32, 400, 47]\n",
      "  y: [32, 1, 23]\n",
      "\n",
      "Epoch: 1 | train_loss: 0.07501, val_loss: 0.07082, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 2 | train_loss: 0.07511, val_loss: 0.06648, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 3 | train_loss: 0.07426, val_loss: 0.06717, lr: 1.00E-08, _patience: 9\n",
      "Epoch: 4 | train_loss: 0.07533, val_loss: 0.06535, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 5 | train_loss: 0.07460, val_loss: 0.06856, lr: 1.00E-08, _patience: 9\n",
      "Epoch: 6 | train_loss: 0.07525, val_loss: 0.06682, lr: 1.00E-08, _patience: 8\n",
      "Epoch: 7 | train_loss: 0.07514, val_loss: 0.06204, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 8 | train_loss: 0.07530, val_loss: 0.07010, lr: 1.00E-08, _patience: 9\n",
      "Epoch: 9 | train_loss: 0.07500, val_loss: 0.06054, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 10 | train_loss: 0.07524, val_loss: 0.07005, lr: 1.00E-08, _patience: 9\n",
      "Epoch: 11 | train_loss: 0.07478, val_loss: 0.06715, lr: 1.00E-08, _patience: 8\n",
      "Epoch: 12 | train_loss: 0.07504, val_loss: 0.06437, lr: 1.00E-08, _patience: 7\n",
      "Epoch: 13 | train_loss: 0.07543, val_loss: 0.06693, lr: 1.00E-08, _patience: 6\n",
      "Epoch: 14 | train_loss: 0.07549, val_loss: 0.06388, lr: 1.00E-08, _patience: 5\n",
      "Epoch: 15 | train_loss: 0.07491, val_loss: 0.06564, lr: 1.00E-08, _patience: 4\n",
      "Epoch: 16 | train_loss: 0.07545, val_loss: 0.06689, lr: 1.00E-08, _patience: 3\n",
      "Epoch: 17 | train_loss: 0.07533, val_loss: 0.06617, lr: 1.00E-08, _patience: 2\n",
      "Epoch: 18 | train_loss: 0.07535, val_loss: 0.07208, lr: 1.00E-08, _patience: 1\n",
      "Stopping early!\n",
      "Performance of 6 fold:\n",
      "Average Training Loss: 0.0751 \t Average Test Loss: 0.0667 \n",
      "Hamming Loss: 0.8791 \t mPrecision: 1.0000 \t mRecall: 0.0823 \t mF1: 0.1520 \t micro_auc: 0.5784 \t macros_auc: 0.7496 \n",
      "Fold 8\n",
      "Sample batch:\n",
      "  X: [32, 400, 47]\n",
      "  y: [32, 1, 23]\n",
      "\n",
      "Epoch: 1 | train_loss: 0.07362, val_loss: 0.08806, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 2 | train_loss: 0.07320, val_loss: 0.08546, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 3 | train_loss: 0.07360, val_loss: 0.08868, lr: 1.00E-08, _patience: 9\n",
      "Epoch: 4 | train_loss: 0.07349, val_loss: 0.08197, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 5 | train_loss: 0.07338, val_loss: 0.08329, lr: 1.00E-08, _patience: 9\n",
      "Epoch: 6 | train_loss: 0.07350, val_loss: 0.07486, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 7 | train_loss: 0.07364, val_loss: 0.08433, lr: 1.00E-08, _patience: 9\n",
      "Epoch: 8 | train_loss: 0.07357, val_loss: 0.07468, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 9 | train_loss: 0.07319, val_loss: 0.08382, lr: 1.00E-08, _patience: 9\n",
      "Epoch: 10 | train_loss: 0.07294, val_loss: 0.09024, lr: 1.00E-08, _patience: 8\n",
      "Epoch: 11 | train_loss: 0.07344, val_loss: 0.08599, lr: 1.00E-08, _patience: 7\n",
      "Epoch: 12 | train_loss: 0.07329, val_loss: 0.07638, lr: 1.00E-08, _patience: 6\n",
      "Epoch: 13 | train_loss: 0.07353, val_loss: 0.08271, lr: 1.00E-08, _patience: 5\n",
      "Epoch: 14 | train_loss: 0.07311, val_loss: 0.08390, lr: 1.00E-08, _patience: 4\n",
      "Epoch: 15 | train_loss: 0.07283, val_loss: 0.08127, lr: 1.00E-08, _patience: 3\n",
      "Epoch: 16 | train_loss: 0.07344, val_loss: 0.08642, lr: 1.00E-08, _patience: 2\n",
      "Epoch: 17 | train_loss: 0.07368, val_loss: 0.08329, lr: 1.00E-08, _patience: 1\n",
      "Stopping early!\n",
      "Performance of 7 fold:\n",
      "Average Training Loss: 0.0734 \t Average Test Loss: 0.0833 \n",
      "Hamming Loss: 0.8519 \t mPrecision: 1.0000 \t mRecall: 0.0998 \t mF1: 0.1815 \t micro_auc: 0.6221 \t macros_auc: 0.7496 \n",
      "Fold 9\n",
      "Sample batch:\n",
      "  X: [32, 400, 47]\n",
      "  y: [32, 1, 23]\n",
      "\n",
      "Epoch: 1 | train_loss: 0.07327, val_loss: 0.08348, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 2 | train_loss: 0.07315, val_loss: 0.07863, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 3 | train_loss: 0.07250, val_loss: 0.08939, lr: 1.00E-08, _patience: 9\n",
      "Epoch: 4 | train_loss: 0.07343, val_loss: 0.08964, lr: 1.00E-08, _patience: 8\n",
      "Epoch: 5 | train_loss: 0.07323, val_loss: 0.09191, lr: 1.00E-08, _patience: 7\n",
      "Epoch: 6 | train_loss: 0.07308, val_loss: 0.09243, lr: 1.00E-08, _patience: 6\n",
      "Epoch: 7 | train_loss: 0.07324, val_loss: 0.08291, lr: 1.00E-08, _patience: 5\n",
      "Epoch: 8 | train_loss: 0.07304, val_loss: 0.08792, lr: 1.00E-08, _patience: 4\n",
      "Epoch: 9 | train_loss: 0.07326, val_loss: 0.08387, lr: 1.00E-08, _patience: 3\n",
      "Epoch: 10 | train_loss: 0.07331, val_loss: 0.09101, lr: 1.00E-08, _patience: 2\n",
      "Epoch: 11 | train_loss: 0.07305, val_loss: 0.07262, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 12 | train_loss: 0.07337, val_loss: 0.08944, lr: 1.00E-08, _patience: 9\n",
      "Epoch: 13 | train_loss: 0.07299, val_loss: 0.09056, lr: 1.00E-08, _patience: 8\n",
      "Epoch: 14 | train_loss: 0.07292, val_loss: 0.07908, lr: 1.00E-08, _patience: 7\n",
      "Epoch: 15 | train_loss: 0.07335, val_loss: 0.08753, lr: 1.00E-08, _patience: 6\n",
      "Epoch: 16 | train_loss: 0.07284, val_loss: 0.08595, lr: 1.00E-08, _patience: 5\n",
      "Epoch: 17 | train_loss: 0.07321, val_loss: 0.08955, lr: 1.00E-08, _patience: 4\n",
      "Epoch: 18 | train_loss: 0.07338, val_loss: 0.08434, lr: 1.00E-08, _patience: 3\n",
      "Epoch: 19 | train_loss: 0.07330, val_loss: 0.08757, lr: 1.00E-08, _patience: 2\n",
      "Epoch: 20 | train_loss: 0.07309, val_loss: 0.08388, lr: 1.00E-08, _patience: 1\n",
      "Stopping early!\n",
      "Performance of 8 fold:\n",
      "Average Training Loss: 0.0732 \t Average Test Loss: 0.0861 \n",
      "Hamming Loss: 0.8567 \t mPrecision: 0.9923 \t mRecall: 0.0929 \t mF1: 0.1698 \t micro_auc: 0.6351 \t macros_auc: 0.7496 \n",
      "Fold 10\n",
      "Sample batch:\n",
      "  X: [32, 400, 47]\n",
      "  y: [32, 1, 23]\n",
      "\n",
      "Epoch: 1 | train_loss: 0.07456, val_loss: 0.06686, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 2 | train_loss: 0.07447, val_loss: 0.07628, lr: 1.00E-08, _patience: 9\n",
      "Epoch: 3 | train_loss: 0.07453, val_loss: 0.07627, lr: 1.00E-08, _patience: 8\n",
      "Epoch: 4 | train_loss: 0.07443, val_loss: 0.07485, lr: 1.00E-08, _patience: 7\n",
      "Epoch: 5 | train_loss: 0.07414, val_loss: 0.07329, lr: 1.00E-08, _patience: 6\n",
      "Epoch: 6 | train_loss: 0.07443, val_loss: 0.07130, lr: 1.00E-08, _patience: 5\n",
      "Epoch: 7 | train_loss: 0.07409, val_loss: 0.07245, lr: 1.00E-08, _patience: 4\n",
      "Epoch: 8 | train_loss: 0.07430, val_loss: 0.07321, lr: 1.00E-08, _patience: 3\n",
      "Epoch: 9 | train_loss: 0.07463, val_loss: 0.06391, lr: 1.00E-08, _patience: 10\n",
      "Epoch: 10 | train_loss: 0.07463, val_loss: 0.07034, lr: 1.00E-08, _patience: 9\n",
      "Epoch: 11 | train_loss: 0.07449, val_loss: 0.07755, lr: 1.00E-08, _patience: 8\n",
      "Epoch: 12 | train_loss: 0.07457, val_loss: 0.07023, lr: 1.00E-08, _patience: 7\n",
      "Epoch: 13 | train_loss: 0.07428, val_loss: 0.06433, lr: 1.00E-08, _patience: 6\n",
      "Epoch: 14 | train_loss: 0.07460, val_loss: 0.06573, lr: 1.00E-08, _patience: 5\n",
      "Epoch: 15 | train_loss: 0.07428, val_loss: 0.06884, lr: 1.00E-08, _patience: 4\n",
      "Epoch: 16 | train_loss: 0.07393, val_loss: 0.07150, lr: 1.00E-08, _patience: 3\n",
      "Epoch: 17 | train_loss: 0.07456, val_loss: 0.07105, lr: 1.00E-08, _patience: 2\n",
      "Epoch: 18 | train_loss: 0.07429, val_loss: 0.07561, lr: 1.00E-08, _patience: 1\n",
      "Stopping early!\n",
      "Performance of 9 fold:\n",
      "Average Training Loss: 0.0744 \t Average Test Loss: 0.0713 \n",
      "Hamming Loss: 0.8635 \t mPrecision: 1.0000 \t mRecall: 0.0947 \t mF1: 0.1731 \t micro_auc: 0.6404 \t macros_auc: 0.7496 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report\n",
    "from sklearn.metrics import hamming_loss, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "k=10\n",
    "splits=KFold(n_splits=k,shuffle=True)\n",
    "\n",
    "\n",
    "# Trainer module\n",
    "trainer = Trainer(\n",
    "    model=model, device=device, loss_fn=loss_fn,\n",
    "    optimizer=optimizer, scheduler=scheduler)\n",
    "\n",
    "    \n",
    "history = {'Hamming loss': [], 'microPrecision': [], 'microRecall': [], 'microF1': [],'micro_auc': [], 'macro_auc': [] }\n",
    "\n",
    "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
    "\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_dataloader = dataset.create_dataloader(batch_size=batch_size, sampler=train_sampler)\n",
    "    val_dataloader = dataset.create_dataloader(batch_size=batch_size, sampler=test_sampler)\n",
    "    batch_X, batch_y = next(iter(val_dataloader))\n",
    "    print (\"Sample batch:\\n\"\n",
    "    f\"  X: {list(batch_X.size())}\\n\"\n",
    "    f\"  y: {list(batch_y.size())}\\n\")\n",
    "        # Train\n",
    "    best_model, avg_train_loss,avg_test_loss = trainer.train(\n",
    "        NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)\n",
    "    \n",
    "    print(f'Performance of {fold} fold:')\n",
    "    print(\"Average Training Loss: {:.4f} \\t Average Test Loss: {:.4f} \".format(avg_train_loss,avg_test_loss)) \n",
    "    \n",
    "    # For evaluation\n",
    "    test_loss, y_true, y_pred_prob = trainer.eval_step(dataloader=val_dataloader)\n",
    "\n",
    "    # 2. Apply thresholding to convert probabilities to binary predictions\n",
    "    threshold = 0.5\n",
    "    y_pred_binary = (y_pred_prob > threshold).astype(float)\n",
    "\n",
    "    # 3. Calculate evaluation metrics\n",
    "    hamming_loss_value = hamming_loss(y_true, y_pred_binary)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred_binary, average='micro')\n",
    "    # Micro-average AUC\n",
    "    try:\n",
    "        micro_auc = roc_auc_score(y_true.ravel(), y_pred_prob.ravel())\n",
    "        macro_auc = roc_auc_score(y_true, y_pred_prob, average='macro')\n",
    "    except ValueError:\n",
    "        pass\n",
    "#     micro_auc = roc_auc_score(y_true, y_pred_prob)\n",
    "\n",
    "#     # Macro-average AUC\n",
    "#     macro_auc = roc_auc_score(y_true, y_pred_prob, average='macro')\n",
    "\n",
    "    history['Hamming loss'].append(hamming_loss_value)\n",
    "    history['microPrecision'].append(precision)\n",
    "    history['microRecall'].append(recall)\n",
    "    history['microF1'].append(f1)\n",
    "    history['micro_auc'].append(micro_auc)\n",
    "    history['macro_auc'].append(macro_auc)\n",
    "    # Print or use the evaluation metrics as needed\n",
    "    print(\"Hamming Loss: {:.4f} \\t mPrecision: {:.4f} \\t mRecall: {:.4f} \\t mF1: {:.4f} \\t micro_auc: {:.4f} \\t macros_auc: {:.4f} \".format(hamming_loss_value,precision,recall,f1,micro_auc,macro_auc))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "id": "eb74aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "id": "af0918f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average performance over all folds: \n",
      "Hamming Loss: 0.8649456521739131\n",
      "Precision: 0.9992307692307693\n",
      "Recall: 0.09043640958314962\n",
      "F1-score: 0.16579009603889885\n",
      "micro_auc: 0.6200477743306908\n",
      "macro_auc: 0.74051493043151\n"
     ]
    }
   ],
   "source": [
    "print(\"Average performance over all folds: \".format(avg_train_loss,avg_test_loss))\n",
    "\n",
    "print(\"Hamming Loss:\", np.mean(history['Hamming loss']))\n",
    "print(\"Precision:\", np.mean(history['microPrecision']))\n",
    "print(\"Recall:\", np.mean(history['microRecall']))\n",
    "print(\"F1-score:\", np.mean(history['microF1']))\n",
    "print(\"micro_auc:\", np.mean(history['micro_auc']))\n",
    "print(\"macro_auc:\", np.mean(history['macro_auc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8af1c1",
   "metadata": {},
   "source": [
    "So recall and F1 is really poor for the whole dataset. But for a side effect predictor, its best to minimize false negatives as we don't want say no side effect when there can be a true side effect that can exist. So need high recall. Also hamming loss is really high. So i believe, for a whole dataset, we need a more complex models like many kernels and filters and hidden layers to improve prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3f86fb",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "id": "ad569de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "id": "117ac202",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [50, 45, 3], expected input[16, 38, 51] to have 45 channels, but got 38 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [859]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hamming_loss, accuracy_score, precision_recall_fscore_support\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 1. Get predictions from the model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m test_loss, y_true, y_pred_prob \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 2. Apply thresholding to convert probabilities to binary predictions\u001b[39;00m\n\u001b[1;32m      8\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n",
      "Input \u001b[0;32mIn [851]\u001b[0m, in \u001b[0;36mTrainer.eval_step\u001b[0;34m(self, dataloader)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 batch \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch]  \u001b[38;5;66;03m# Set device\u001b[39;00m\n\u001b[1;32m     48\u001b[0m                 inputs, y_true \u001b[38;5;241m=\u001b[39m batch[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], batch[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 50\u001b[0m                 z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#                 J = self.loss_fn(z, y_true).item()\u001b[39;00m\n\u001b[1;32m     52\u001b[0m                 J \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(z, y_true\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [815]\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, inputs, channel_first)\u001b[0m\n\u001b[1;32m     31\u001b[0m padding_right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(math\u001b[38;5;241m.\u001b[39mceil((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv\u001b[38;5;241m.\u001b[39mstride[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39m(max_seq_len\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m max_seq_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_size)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Conv outputs\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadding_left\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_right\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_norm(z)  \u001b[38;5;66;03m# Batch normalization\u001b[39;00m\n\u001b[1;32m     36\u001b[0m z \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(z)  \u001b[38;5;66;03m# Activation function\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:307\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:303\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    301\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    302\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [50, 45, 3], expected input[16, 38, 51] to have 45 channels, but got 38 channels instead"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import hamming_loss, accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# 1. Get predictions from the model\n",
    "test_loss, y_true, y_pred_prob = trainer.eval_step(dataloader=test_dataloader)\n",
    "\n",
    "\n",
    "# 2. Apply thresholding to convert probabilities to binary predictions\n",
    "threshold = 0.5\n",
    "# threshold = 0.043 # 1/23 classes\n",
    "# y_pred_binary = (y_pred_prob > threshold).astype(int)\n",
    "y_pred_binary = (y_pred_prob > threshold).astype(float)\n",
    "\n",
    "print(y_true.shape, y_pred_prob.shape, y_pred_binary.shape)\n",
    "for i in range(y_true.shape[0]):\n",
    "    if np.array_equal(y_true[i], y_pred_binary[i]):\n",
    "        print(y_true[i], y_pred_binary[i])\n",
    "\n",
    "# print(y_true[i], y_pred_binary[i]) for i in range(y_true.shape[0]) if y_true[i] == y_pred_binary[i] \n",
    "\n",
    "# 3. Calculate evaluation metrics\n",
    "hamming_loss_value = hamming_loss(y_true, y_pred_binary)\n",
    "accuracy = accuracy_score(y_true, y_pred_binary)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred_binary, average='micro')\n",
    "\n",
    "# Print or use the evaluation metrics as needed\n",
    "print(\"Hamming Loss:\", hamming_loss_value)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "id": "142169d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(y_true.shape[0]):\n",
    "#     print(y_true[i], y_pred_binary[i])\n",
    "#     print(y_pred_prob[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea1d16f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
